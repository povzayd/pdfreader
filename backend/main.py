from fastapi import FastAPI, UploadFile, BackgroundTasks
from fastapi.responses import Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
from dotenv import load_dotenv

# Import our modules
from rag_engine import initialize_rag, ask_question, generate_summary
from audio_gen import generate_podcast_dialogue

load_dotenv()

app = FastAPI()

# Allow CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize RAG on startup
@app.on_event("startup")
async def startup_event():
    # In production, use background task to not block boot
    initialize_rag()

class QueryRequest(BaseModel):
    query: str

@app.post("/chat")
def chat_endpoint(request: QueryRequest):
    answer = ask_question(request.query)
    return {"response": answer}

@app.get("/summary")
def summary_endpoint():
    tips = generate_summary()
    return {"tips": tips}

@app.get("/podcast")
def podcast_endpoint(topic: str = "General Economics"):
    # Returns list of text segments. 
    # Frontend will request specific audio blobs by index to play them.
    # For MVP simplicity, we generate fresh every time.
    dialogue = generate_podcast_dialogue(topic)
    # We can't send bytes JSON easily, so we'll save temp files or base64.
    # Strategy: Return text transcript, separate endpoint for audio blob.
    
    import base64
    response_data = []
    for seg in dialogue:
        b64_audio = base64.b64encode(seg['audio']).decode('utf-8')
        response_data.append({
            "speaker": seg['speaker'],
            "text": seg['text'],
            "audio_base64": b64_audio
        })
        
    return response_data
    '''
def initialize_rag():
    global vector_store, qa_chain
    print("ğŸš€ Starting RAG Initialization...") # ADD THIS
    
    docs = []
    
    if os.path.exists(PDF_PATH):
        print(f"ğŸ“– Loading PDF from {PDF_PATH}...") # ADD THIS
        loader = PyPDFLoader(PDF_PATH)
        docs.extend(loader.load())
        print(f"âœ… PDF Loaded. Total pages: {len(docs)}") # ADD THIS
    else:
        print(f"âŒ ERROR: {PDF_PATH} not found!") 

    print("ğŸ¥ Loading YouTube transcripts...") # ADD THIS
    # ... (rest of your loading code)
    
    print("ğŸ§  Creating Vector Store (this might take a minute)...") # ADD THIS
    vector_store = FAISS.from_documents(splits, embeddings)
    
    print("ğŸ¤– Setting up Chat Chain...") # ADD THIS
    # ... (rest of your chain code)
    
    print("âœ¨ RAG System Ready!") 
    '''
    # ADD THIS
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)









    '''''
/ecostudy-assignment
â”œâ”€â”€ /backend                 # Python FastAPI Backend
â”‚   â”œâ”€â”€ .env                 # Store your OPENAI_API_KEY here
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies list
â”‚   â”œâ”€â”€ main.py              # The API entry point (FastAPI app)
â”‚   â”œâ”€â”€ rag_engine.py        # Logic for PDF/YouTube processing & RAG
â”‚   â”œâ”€â”€ audio_gen.py         # Logic for generating the Podcast audio
â”‚   â””â”€â”€ economics_chapter.pdf # The downloaded PDF file (You must place this here!)
â”‚
â””â”€â”€ /frontend                # Next.js React Frontend
    â”œâ”€â”€ /app
    â”‚   â”œâ”€â”€ globals.css      # Global styles (Tailwind directives)
    â”‚   â”œâ”€â”€ layout.tsx       # Main layout wrapper
    â”‚   â””â”€â”€ page.tsx         # The main UI code (Chat & Audio interface)
    â”œâ”€â”€ /public              # Static assets (images, icons)
    â”œâ”€â”€ package.json         # Node dependencies list
    â”œâ”€â”€ tailwind.config.ts   # Tailwind configuration
    â”œâ”€â”€ next.config.js       # Next.js configuration
    â””â”€â”€ tsconfig.json        # TypeScript configuration

    '''